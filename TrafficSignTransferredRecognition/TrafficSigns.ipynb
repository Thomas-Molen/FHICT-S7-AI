{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources Used  \n",
    "[Traffic sign dataset](https://www.kaggle.com/datasets/ahemateja19bec1025/traffic-sign-dataset-classification)\n",
    "[Example notebook on traffic sign dataset](https://www.kaggle.com/code/moh3we5/traffic-sign-dataset-resnet-classification)\n",
    "[Tensor flow transfer learning documentation](https://www.tensorflow.org/guide/keras/transfer_learning)\n",
    "\n",
    "### Introduction\n",
    "A Traffic sign dataset will be used, to create a CNN capable of recognizing common traffic signs, these datasets are often used as a component of self driving AI. While most of those implementations tend to use multiple models to narrow down the kind of sign, thereby improving possible classification even with small deviations between similar signs.  \n",
    "In this notebook a single large model will be trained however.\n",
    "\n",
    "The dataset used contains 58 classes, covering most common signs, however some classes are labeled as Unkown[n].\n",
    "For every class ~120 images are present.\n",
    "\n",
    "In this notebook, transfer learning will be leveraged to obtain a pretrained base model. By using transfer learning, the training process can be greatly reduced and will allow us to utilize layers that have been trained on much larger datasets than the one used here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the traffic sign dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = \"Dataset/traffic_Data/DATA/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.keras.utils.image_dataset_from_directory(dataset_directory, labels='inferred', seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, obtain a pre trained model, this can be done via the tensorflow keras API.  \n",
    "Here we specify a model that has its weights trained on the ImageNet dataset, the expected image input size and that the classification top layer should be excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "83683744/83683744 [==============================] - 7s 0us/step\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = keras.applications.Xception(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet\n",
    "    input_shape=(150, 150, 3), # Input shape of our dataset images\n",
    "    include_top=False)  # Exclude the classifier layer, as our own will be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Block the pretrained layers/base-model from having its weights changed during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets create our own model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(150, 150, 3))\n",
    "# We make sure that the base_model is running in inference mode here,\n",
    "# by passing `training=False`. This is important for fine-tuning, as you will\n",
    "# learn in a few paragraphs.\n",
    "x = pretrained_model(inputs, training=False)\n",
    "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "# A Dense classifier with a single unit (binary classification)\n",
    "outputs = keras.layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train this new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])\n",
    "model.fit(new_dataset, epochs=20, callbacks=..., validation_data=...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
